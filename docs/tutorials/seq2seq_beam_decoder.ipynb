{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_beam_decoder.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kazemnejad/addons/blob/seq2seq_example_draft/docs/tutorials/seq2seq_beam_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tuOe1ymfHZPu",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN",
        "colab_type": "text"
      },
      "source": [
        "# Seq2seq: Neural Machine Translation with Beam search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/kazemnejad/addons/blob/seq2seq_example/examples/seq2seq_beam_decoder.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/kazemnejad/addons/blob/seq2seq_example/examples/seq2seq_beam_decoder.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6P32iYYV27b"
      },
      "source": [
        "[Update button links]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Overview\n",
        "\n",
        "[Include a paragraph or two explaining what this example demonstrates, who should be interested in it, and what you need to know before you get started.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n8UUkG9rx68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install -q  --no-deps -e tfa-nightly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYsDcEND-_dC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install tf-addons-nightly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SqUrkNVXM2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IqR2PQG4ZaZ0",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  pass\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from tensorflow_datasets.core.features.text import TokenTextEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzoqatFD9dw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp -r /content/addons/bazel-bin/tensorflow_addons /content/addons/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg09oUtYVlao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "EMBED_DIM = 100\n",
        "HIDDEN_DIM = 128\n",
        "NUM_LAYERS = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pxsfUyweenG",
        "colab_type": "text"
      },
      "source": [
        "## Prepare the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ8YIfJPWTvl",
        "colab_type": "text"
      },
      "source": [
        "Download and prepare a toy machine translation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufaOSuhMyIiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select a small percentage of the dataset for training and testing.\n",
        "train_split = tfds.Split.TRAIN.subsplit(tfds.percent[:5])\n",
        "test_split = tfds.Split.TRAIN.subsplit(tfds.percent[-1:])\n",
        "(dataset_train, dataset_test), info = tfds.load(\n",
        "    name=\"para_crawl/enmt_plain_text\",\n",
        "    split=(train_split, test_split),\n",
        "    with_info=True)\n",
        "# Cache the language pair to use it later\n",
        "lang_src, lang_tgt = info.supervised_keys[::-1]\n",
        "print(f\"lang_src, lang_tgt = {lang_src}, {lang_tgt}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFsrqtVYaOTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an instance of tokenizer. Special tokens should be also provided,\n",
        "# as this tokenizer instance will be used in the `TokenTextEncoder`\n",
        "# to replace words with their vocabulary ids.\n",
        "tokenizer = tfds.features.text.Tokenizer(reserved_tokens=['<s>', '</s>'])\n",
        "# Create a vocabulary for both language pairs.\n",
        "vocab_src, vocab_tgt = Counter(), Counter()\n",
        "for d in dataset_train:\n",
        "    tokens = tokenizer.tokenize(d[lang_src].numpy())\n",
        "    vocab_src.update(tokens)\n",
        "    tokens = tokenizer.tokenize(d[lang_tgt].numpy())\n",
        "    vocab_tgt.update(tokens)\n",
        "\n",
        "# Cut down the vocabulary size to a pre-defined number.\n",
        "vocab_src = [w for w, _ in vocab_src.most_common(VOCAB_SIZE)]\n",
        "vocab_tgt = ['<s>', '</s>'] + [w for w, _ in vocab_tgt.most_common(VOCAB_SIZE)]\n",
        "# To convert words to their corresponding ids, you should \n",
        "# create two separate instance of `TokenTextEncoder` for each\n",
        "# language\n",
        "tok_encoder_src = TokenTextEncoder(\n",
        "    vocab_src, lowercase=True, tokenizer=tokenizer)\n",
        "tok_encoder_tgt = TokenTextEncoder(\n",
        "    vocab_tgt, lowercase=True, tokenizer=tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvFt9HtqegLZ",
        "colab_type": "text"
      },
      "source": [
        "As for the preprocessing stage, the following steps should be done:\n",
        "- Replace each word with its id from the vocabulary table.\n",
        "- `<s>` and `</s>` should be added to the begging and end of each target sentence, respectively.\n",
        "- Pad all setences within each batch to have the same length.\n",
        "- Convert the labels to the one-hot format.\n",
        "- Compute the `sample_weights` mask according to the padded batch to zero out the effect of padding values in the loss calculation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCFOBTyJuuxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _encode_str(src, tgt):\n",
        "    src_word_ids = tok_encoder_src.encode(src.numpy())\n",
        "    tgt_input_word_ids = tok_encoder_tgt.encode(\n",
        "        '<s> '.encode('utf8') + tgt.numpy())\n",
        "    tgt_output_word_ids = tok_encoder_tgt.encode(\n",
        "        tgt.numpy() + ' </s>'.encode('utf8'))\n",
        "    return src_word_ids, tgt_input_word_ids, tgt_output_word_ids\n",
        "\n",
        "\n",
        "def _encode_map_fn(src, tgt):\n",
        "    return tf.py_function(\n",
        "        _encode_str, inp=(src, tgt), Tout=(tf.int64, tf.int64, tf.int64))\n",
        "\n",
        "\n",
        "def _append_sample_weights(src, tgt_inp, tgt_out):\n",
        "    weights = tf.cast(tf.not_equal(tgt_out, 0), tf.float32)\n",
        "    return (src, tgt_inp), tgt_out, weights\n",
        "\n",
        "\n",
        "def _convert_labels_to_one_hot(inputs, labels, sample_weights):\n",
        "    labels = tf.one_hot(labels, depth=tok_encoder_tgt.vocab_size)\n",
        "    return inputs, labels, sample_weights\n",
        "\n",
        "\n",
        "def prepare_dataset(dataset):\n",
        "    dataset = dataset.map(lambda x: (x[lang_src], x[lang_tgt]))\n",
        "    # shape: ([None], [None], [None]), dtype: (tf.int64, tf.int64, tf.int64)\n",
        "    dataset = dataset.map(_encode_map_fn)\n",
        "\n",
        "    pad_value = tf.constant(0, dtype=tf.int64)\n",
        "    # shape: ([batch_size, None], [batch_size, None], [batch_size, None]),\n",
        "    dataset = dataset.padded_batch(\n",
        "        BATCH_SIZE,\n",
        "        padded_shapes=([None], [None], [None]),\n",
        "        padding_values=(pad_value, pad_value, pad_value))\n",
        "    \n",
        "    # Finally, each row is an (input, labels, sample_weights)-triple,\n",
        "    # where the input itself is a (source, shifted_target)-tuple\n",
        "    # So the final shape is: (\n",
        "    #     ([batch_size, max_seq_len], [batch_size, max_seq_len]),\n",
        "    #     [batch_size, max_seq_len],\n",
        "    #     [batch_size, max_seq_len]\n",
        "    # )\n",
        "    dataset = dataset.map(_append_sample_weights)\n",
        "\n",
        "    # To make the dataset compatible with Keras built-in training loops,\n",
        "    # labels should be provided in the one-hot format. So the labels' shape is:\n",
        "    # [batch_size, max_seq_len, vocab_size]\n",
        "    dataset = dataset.map(_convert_labels_to_one_hot)\n",
        "    \n",
        "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAn5dMzIoynt",
        "colab_type": "text"
      },
      "source": [
        "And finally, apply those transformations on both splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS1D8Jju-c_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds_train = prepare_dataset(dataset_train)\n",
        "ds_valid = prepare_dataset(dataset_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3gfBA9DL_By",
        "colab_type": "text"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GKsUByVr8Xr",
        "colab_type": "text"
      },
      "source": [
        "We'll use model-subclassing to define the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W71KzUntMA4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2SeqModel(tf.keras.models.Model):\n",
        "    def __init__(self):\n",
        "        super(Seq2SeqModel, self).__init__()\n",
        "        # Define the embedding layers, one for the source language\n",
        "        # and one for the target.\n",
        "        self.src_embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=tok_encoder_src.vocab_size,\n",
        "            output_dim=EMBED_DIM,\n",
        "            mask_zero=True)\n",
        "        self.tgt_embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=tok_encoder_tgt.vocab_size,\n",
        "            output_dim=EMBED_DIM,\n",
        "            mask_zero=True)\n",
        "        \n",
        "        # Create a multi-layer LSTM encoder.\n",
        "        self.encoder = tf.keras.layers.RNN(\n",
        "            tf.keras.layers.StackedRNNCells([\n",
        "                tf.keras.layers.LSTMCell(HIDDEN_DIM) for _ in range(NUM_LAYERS)\n",
        "            ]),\n",
        "            return_sequences=True,\n",
        "            return_state=True)\n",
        "\n",
        "        # Create an Attention Mechanism with HIDDEN_DIM as \n",
        "        # the depth of the query mechanism. The memory is set \n",
        "        # in the actual model invocation.\n",
        "        self.attn_mch = tfa.seq2seq.LuongAttention(HIDDEN_DIM)\n",
        "\n",
        "        # The decoder is also a multi-layer LSTM network, however \n",
        "        # we should keep a reference to its cell in order to use that \n",
        "        # in both Train(Basic) & inference(BeamSearch) Decoders\n",
        "        self.decoder_cell = tf.keras.layers.StackedRNNCells(\n",
        "            [tf.keras.layers.LSTMCell(HIDDEN_DIM) for _ in range(NUM_LAYERS)])\n",
        "        \n",
        "        # Wrap the cell to add the attention functionality\n",
        "        self.decoder_cell = tfa.seq2seq.AttentionWrapper(\n",
        "            cell=self.decoder_cell,\n",
        "            attention_mechanism=self.attn_mch,\n",
        "            alignment_history=False)\n",
        "        \n",
        "        # Create a Dense layer for the vocabulary projection\n",
        "        vocab_proj_layer = tf.keras.layers.Dense(tok_encoder_tgt.vocab_size)\n",
        "\n",
        "        # Create an instance of BasicDecoder to be used in the training time.\n",
        "        self.train_decoder = tfa.seq2seq.BasicDecoder(\n",
        "            cell=self.decoder_cell,\n",
        "            sampler=tfa.seq2seq.sampler.TrainingSampler(),\n",
        "            output_layer=vocab_proj_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veifcr3Nwd7W",
        "colab_type": "text"
      },
      "source": [
        "Utilize the LSTM encoder to create a represention of the input sentence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UNjwnCWrJOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _encode_input(self, src_words, training):\n",
        "    embeds = self.src_embedding(src_words)\n",
        "    mask = self.src_embedding.compute_mask(src_words)\n",
        "    encoder_outputs, state_h, state_c = self.encoder(embeds, mask=mask, training=training)\n",
        "    return encoder_outputs, mask, (state_h, state_c)\n",
        "\n",
        "Seq2SeqModel._encode_input = _encode_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr_ZN19WrFGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _decode(self, encoder_outputs, tgt_input_words, training):\n",
        "    tgt_input_embeds = self.tgt_embedding(tgt_input_words)\n",
        "    tgt_input_mask = self.tgt_embedding.compute_mask(tgt_input_words)\n",
        "    \n",
        "    enc_hiddens, enc_hiddens_mask, enc_final_state = encoder_outputs\n",
        "    self.attn_mch(enc_hiddens, memory_mask=enc_hiddens_mask, setup_memory=True)\n",
        "\n",
        "    decoder_initial_state = self.decoder_cell.get_initial_state(\n",
        "        tgt_input_embeds)\n",
        "    decoder_initial_state = decoder_initial_state.clone(\n",
        "        cell_state=enc_final_state)\n",
        "\n",
        "    outputs, _, _ = self.train_decoder(\n",
        "        tgt_input_embeds,\n",
        "        initial_state=decoder_initial_state,\n",
        "        training=training,\n",
        "        mask=tgt_input_mask)\n",
        "\n",
        "    logits = outputs.rnn_output\n",
        "\n",
        "    return logits\n",
        "\n",
        "Seq2SeqModel._decode = _decode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSleUrp2xD8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def call(self, inputs, training=None):\n",
        "    src_words, tgt_input_words = inputs\n",
        "\n",
        "    encoder_outputs = self._encode_input(src_words, training)\n",
        "\n",
        "    logits = self._decode(encoder_outputs, tgt_input_words, training)\n",
        "\n",
        "    return logits\n",
        "\n",
        "Seq2SeqModel.call = call"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM8rLW0WTOcF",
        "colab_type": "text"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6Q3i_QM3zcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds_train = prepare_dataset(dataset_train)\n",
        "ds_valid = prepare_dataset(dataset_test)\n",
        "\n",
        "model2 = Seq2SeqModel()\n",
        "model2.compile(optimizer='adam',\n",
        "              loss=tfa.seq2seq.SequenceLoss(),\n",
        "              sample_weight_mode=\"temporal\")\n",
        "model2.fit(ds_train, epochs=1, steps_per_epoch=100000, validation_data=ds_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KtylpxOmceaC",
        "colab": {}
      },
      "source": [
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(None, 5)),\n",
        "    tf.keras.layers.Dense(3)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YrsKXcPRUvK9"
      },
      "source": [
        "## GitHub workflow\n",
        "\n",
        "* Be consistent about how you save your notebooks, otherwise the JSON diffs are messy.\n",
        "* This notebook has the \"Omit code cell output when saving this notebook\" option set. GitHub refuses to diff notebooks with large diffs (inline images).\n",
        "* [ReviewNB.com](http://reviewnb.com) can help with diffs. This is linked in a comment on a notebook pull request.\n",
        "* Use the [Open in Colab](https://chrome.google.com/webstore/detail/open-in-colab/iogfkhleblhcpcekbiedikdehleodpjo) extension to open a GitHub notebook in Colab.\n",
        "* The easiest way to edit a notebook in GitHub is to open it with Colab from the branch you want to edit. Then use File --> Save a copy in GitHub, which will save it back to the branch you opened it from.\n",
        "* For PRs it's helpful to post a direct Colab link to the PR head: https://colab.research.google.com/github/{USER}/{REPO}/blob/{BRANCH}/{PATH}.ipynb"
      ]
    }
  ]
}